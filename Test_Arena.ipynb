{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter List\n",
    "parameter_list = {}\n",
    "\n",
    "parameter_list['netCDf_loc'] = \"./lorenz96_multi/DATA_sample/X40F18/all_10/nocorr_I20/assim.nc\"\n",
    "parameter_list['locality'] = 5\n",
    "parameter_list['time_splits'] = 30\n",
    "parameter_list['batch_size'] = 240\n",
    "parameter_list['val_size'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the NetCDF files\n",
    "root_grp = Dataset(parameter_list['netCDf_loc'], \"r\", format=\"NETCDF4\")\n",
    "\n",
    "#Extrating the datasets\n",
    "analysis_init = root_grp[\"vam\"]\n",
    "forecast_init = root_grp[\"vfm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of datasets for training and validation\n",
    "\n",
    "#Creating locality for individual state variable\n",
    "def locality_creator(init_dataset):\n",
    "    \n",
    "    output_dataset = np.zeros((init_dataset.shape[0], init_dataset.shape[1], parameter_list['locality']))\n",
    "    radius = int(parameter_list['locality'] / 2)\n",
    "    \n",
    "    for i in range(init_dataset.shape[1]):\n",
    "        start = i - radius\n",
    "        stop = i + radius\n",
    "        index = np.linspace(start,stop,parameter_list['locality'], dtype='int')\n",
    "        if stop >= init_dataset.shape[1]:\n",
    "            stop2 = (stop + 1)%init_dataset.shape[1]\n",
    "            index[:-stop2] = np.linspace(start,analysis_init.shape[1]-1,analysis_init.shape[1]-start, dtype='int')\n",
    "            index[-stop2:] = np.arange(0,stop2,1,dtype='int')\n",
    "        output_dataset[:,i,:] = init_dataset[:,index]\n",
    "\n",
    "    return np.transpose(output_dataset,(1,0,2)).astype('float32')\n",
    "\n",
    "analysis_dataset = locality_creator(analysis_init)\n",
    "forecast_dataset = locality_creator(forecast_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating time data splits\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X = list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i*n_steps + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[i*n_steps:end_ix, :]\n",
    "        X.append(seq_x)\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For serializing the tensor to a string for TFRecord\n",
    "def _serialize_tensor(value):\n",
    "    return tf.io.serialize_tensor(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For writing data to the TFRecord file\n",
    "def write_TFRecord(filename, dataset):\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for i in range(dataset.shape[0]):\n",
    "            dataset_splits = split_sequences(dataset[i],parameter_list['time_splits'])\n",
    "            for j in range(dataset_splits.shape[0]):\n",
    "                data = dataset_splits[j]\n",
    "                serial_string = _serialize_tensor(data)\n",
    "                writer.write(serial_string.numpy())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For reading the TFRecord File\n",
    "def read_TFRecord(filename):\n",
    "    return tf.data.TFRecordDataset(filename)\n",
    "\n",
    "#For parsing the value from string to float32\n",
    "def _parse_tensor(value):\n",
    "    return tf.io.parse_tensor(value, out_type=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_TFRecord('analysis.tfrecord', analysis_dataset)\n",
    "write_TFRecord('forecast.tfrecord', forecast_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the TFRecord files\n",
    "anal_file = read_TFRecord('analysis.tfrecord')\n",
    "fore_file = read_TFRecord('forecast.tfrecord')\n",
    "\n",
    "#Parsing the dataset\n",
    "anal_file = anal_file.map(_parse_tensor)\n",
    "fore_file = fore_file.map(_parse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zipping the files\n",
    "dataset = tf.data.Dataset.zip((anal_file, fore_file))\n",
    "\n",
    "#Shuffling the dataset\n",
    "dataset = dataset.shuffle(100000)\n",
    "dataset = dataset.batch(batch_size=parameter_list['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-1.7398463  -3.854305    2.9147084  16.174902    5.367921  ]\n",
      "  [-4.6436844  -2.077001    0.65262926 15.686306    1.3139693 ]\n",
      "  [-4.058007   -0.788952    0.8208224  16.103973   -1.8078716 ]\n",
      "  ...\n",
      "  [ 8.991234   14.651135   -0.8641131   9.0588455   5.509129  ]\n",
      "  [ 8.639453   15.103537    0.684075    8.81687     5.3432107 ]\n",
      "  [ 8.833881   14.964432    1.0165862   7.9007444   6.395882  ]]\n",
      "\n",
      " [[ 9.5995455  10.341174    0.05193586 -8.029909    3.750391  ]\n",
      "  [10.430953    6.9138894  -4.4073424  -5.5132213   0.25255945]\n",
      "  [10.695385    0.08031861 -5.9061074  -2.7605069  -2.7055893 ]\n",
      "  ...\n",
      "  [13.661081    3.9851658   2.7625027   5.3202868   7.7867455 ]\n",
      "  [13.499271    2.283166    2.1953018   6.693972   10.515221  ]\n",
      "  [12.797866    1.2229387   2.23049     7.863687   11.2533    ]]\n",
      "\n",
      " [[ 5.485619   11.169187    5.6348686  -2.9273381  -5.6248546 ]\n",
      "  [ 6.379557   11.873319    2.0390596  -3.5120602  -4.1368504 ]\n",
      "  [ 6.7938714  11.4493265  -3.14341    -2.9815614  -2.8750167 ]\n",
      "  ...\n",
      "  [-9.040581    3.8818326  12.145784    5.38488     2.8815272 ]\n",
      "  [-8.874916   -0.30910397 13.248167    6.2828407   1.9206842 ]\n",
      "  [-4.9282784  -4.2640634  11.81125     8.537721    1.2179493 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 5.2975793  15.016719   10.333396   -7.223667    5.5912004 ]\n",
      "  [ 8.781543   15.000679   -1.8510866  -8.182521    2.6824195 ]\n",
      "  [11.471823    9.683332   -9.570142   -3.749931   -1.1910967 ]\n",
      "  ...\n",
      "  [-6.7638345   7.935997   11.118583    5.7977676   7.457766  ]\n",
      "  [-5.9415026   5.336271   14.873251    6.1305156   4.705829  ]\n",
      "  [-4.2386518   3.999105   16.685406    5.15825     3.069558  ]]\n",
      "\n",
      " [[-2.5417042   1.7070849  14.611704   -0.32019532 -2.9879663 ]\n",
      "  [-1.4362063   1.8995317  14.108903   -1.9913096  -1.4624814 ]\n",
      "  [-1.1270449   2.5808623  14.107256   -2.7404883   1.0110333 ]\n",
      "  ...\n",
      "  [ 4.5991435  -5.7347827  -4.480543   -3.3653297  10.040803  ]\n",
      "  [-0.9574631  -5.200264   -1.7431036  -3.040993    9.823466  ]\n",
      "  [-5.942428   -1.4662238  -0.6959305  -1.9119312   8.840944  ]]\n",
      "\n",
      " [[-1.2443385   1.9514265   3.8505871   2.7309225   3.5810888 ]\n",
      "  [ 0.963433    2.5011606   4.7949386   3.0781124   4.194657  ]\n",
      "  [ 3.8909938   3.273123    5.6023703   4.932929    5.749497  ]\n",
      "  ...\n",
      "  [ 7.5831685  10.108918   -1.6801745  -5.9673553   1.3944937 ]\n",
      "  [ 9.868315    5.7388024  -5.9670343  -2.864917   -1.7189113 ]\n",
      "  [ 9.537701   -0.05774717 -6.496972    0.1348527  -2.235322  ]]], shape=(240, 30, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for i,j in dataset.take(1):\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For creating Train and Validation datasets\n",
    "\n",
    "def train_val_creator(dataset, val_size):\n",
    "    val_dataset = dataset.take(val_size)\n",
    "    train_dataset = dataset.skip(val_size)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_val_creator(dataset, parameter_list['val_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
