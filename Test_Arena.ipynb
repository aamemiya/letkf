{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter List\n",
    "parameter_list = {}\n",
    "\n",
    "parameter_list['netCDf_loc'] = \"./lorenz96_multi/DATA_sample/X40F18/all_10/nocorr_I20/assim.nc\"\n",
    "parameter_list['xlocal'] = 3\n",
    "parameter_list['locality'] = 19\n",
    "parameter_list['time_splits'] = 30\n",
    "parameter_list['batch_size'] = 240\n",
    "parameter_list['val_size'] = 2\n",
    "parameter_list['LSTM_output'] = 5\n",
    "parameter_list['net_output'] = 1\n",
    "parameter_list['learning_rate'] = 1e-3\n",
    "parameter_list['log_dir'] = './log'\n",
    "parameter_list['checkpoint_dir'] = './checkpoint'\n",
    "parameter_list['max_checkpoint_keep'] = 4\n",
    "parameter_list['epochs'] = 6\n",
    "parameter_list['log_freq'] = 5\n",
    "parameter_list['early_stop_patience'] = 5\n",
    "parameter_list['num_epochs_checkpoint'] = 2\n",
    "parameter_list['summery_freq'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the NetCDF files\n",
    "root_grp = Dataset(parameter_list['netCDf_loc'], \"r\", format=\"NETCDF4\")\n",
    "\n",
    "#Extrating the datasets\n",
    "analysis_init = root_grp[\"vam\"]\n",
    "forecast_init = root_grp[\"vfm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of datasets for training and validation\n",
    "\n",
    "#For creating locality for individual state variable\n",
    "def locality_creator(init_dataset):\n",
    "    \n",
    "    output_dataset = np.zeros((init_dataset.shape[0], init_dataset.shape[1], parameter_list['locality']))\n",
    "    radius = int(parameter_list['locality'] / 2)\n",
    "    \n",
    "    locality = np.linspace(-radius, radius, parameter_list['locality'])\n",
    "    locality = np.true_divide(locality, parameter_list['xlocal'])\n",
    "    locality = np.power(locality, 2)\n",
    "    locality = np.exp((-1/2) * locality)\n",
    "    \n",
    "    for i in range(init_dataset.shape[1]):\n",
    "        start = i - radius\n",
    "        stop = i + radius\n",
    "        index = np.linspace(start,stop,parameter_list['locality'], dtype='int')\n",
    "        if stop >= init_dataset.shape[1]:\n",
    "            stop2 = (stop + 1)%init_dataset.shape[1]\n",
    "            index[:-stop2] = np.linspace(start,analysis_init.shape[1]-1,analysis_init.shape[1]-start, dtype='int')\n",
    "            index[-stop2:] = np.arange(0,stop2,1,dtype='int')\n",
    "        output_dataset[:,i,:] = init_dataset[:,index]\n",
    "\n",
    "    return np.multiply(np.transpose(output_dataset,(1,0,2)), locality).astype('float32') \n",
    "\n",
    "#For creating the truth label\n",
    "def truth_label_creator(init_dataset):\n",
    "    output_dataset = init_dataset[:]\n",
    "    output_dataset = np.expand_dims(output_dataset, axis=0)\n",
    "    return np.transpose(output_dataset.astype('float32'))\n",
    "\n",
    "analysis_dataset = truth_label_creator(analysis_init)\n",
    "forecast_dataset = locality_creator(forecast_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating time data splits\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X = list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i*n_steps + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x = sequences[i*n_steps:end_ix, :]\n",
    "        X.append(seq_x)\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For serializing the tensor to a string for TFRecord\n",
    "def _serialize_tensor(value):\n",
    "    return tf.io.serialize_tensor(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For writing data to the TFRecord file\n",
    "def write_TFRecord(filename, dataset):\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        for i in range(dataset.shape[0]):\n",
    "            dataset_splits = split_sequences(dataset[i],parameter_list['time_splits'])\n",
    "            for j in range(dataset_splits.shape[0]):\n",
    "                data = dataset_splits[j]\n",
    "                serial_string = _serialize_tensor(data)\n",
    "                writer.write(serial_string.numpy())\n",
    "                count = count + 1\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For reading the TFRecord File\n",
    "def read_TFRecord(filename):\n",
    "    return tf.data.TFRecordDataset(filename)\n",
    "\n",
    "#For parsing the value from string to float32\n",
    "def _parse_tensor(value):\n",
    "    return tf.io.parse_tensor(value, out_type=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_TFRecord('analysis.tfrecord', analysis_dataset)\n",
    "write_TFRecord('forecast.tfrecord', forecast_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the TFRecord files\n",
    "anal_file = read_TFRecord('analysis.tfrecord')\n",
    "fore_file = read_TFRecord('forecast.tfrecord')\n",
    "\n",
    "#Parsing the dataset\n",
    "anal_file = anal_file.map(_parse_tensor)\n",
    "fore_file = fore_file.map(_parse_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zipping the files\n",
    "dataset = tf.data.Dataset.zip((fore_file, anal_file))\n",
    "\n",
    "#Shuffling the dataset\n",
    "dataset = dataset.shuffle(100000)\n",
    "dataset = dataset.batch(batch_size=parameter_list['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For creating Train and Validation datasets\n",
    "\n",
    "def train_val_creator(dataset, val_size):\n",
    "    val_dataset = dataset.take(val_size)\n",
    "    train_dataset = dataset.skip(val_size)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_val_creator(dataset, parameter_list['val_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model defination\n",
    "def rnn_model():\n",
    "    net_input = tf.keras.Input(shape=(None, parameter_list['locality']), name='INPUT')\n",
    "    x = tf.keras.layers.LSTM(units=parameter_list['LSTM_output'], return_sequences=True)(net_input)\n",
    "    output = tf.keras.layers.Dense(units=parameter_list['net_output'], activation=tf.keras.activations.relu)(x)\n",
    "    return tf.keras.Model(net_input, output, name='RNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Model compiling parameters\n",
    "learning_rate = parameter_list['learning_rate']\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, amsgrad=True)\n",
    "\n",
    "#Loss and metric\n",
    "loss_func = tf.keras.losses.MeanSquaredError(name='Loss: MSE')\n",
    "metric_train = tf.keras.metrics.RootMeanSquaredError(name='T_RMSE')\n",
    "metric_val = tf.keras.metrics.RootMeanSquaredError(name='V_RMSE')\n",
    "\n",
    "#Creating summary writer\n",
    "summary_writer = tf.summary.create_file_writer(logdir= parameter_list['log_dir'])\n",
    "\n",
    "#Creating checkpoint instance\n",
    "checkpoint = tf.train.Checkpoint(epoch = tf.Variable(0), optimizer = optimizer, model = model)\n",
    "save_directory = parameter_list['checkpoint_dir']\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory= save_directory, \n",
    "                                    max_to_keep= parameter_list['max_checkpoint_keep'])\n",
    "checkpoint.restore(manager.latest_checkpoint)\n",
    "\n",
    "#Checking if previous checkpoint exists\n",
    "if manager.latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")\n",
    "    \n",
    "#Initialing training variables\n",
    "global_step = 0\n",
    "global_step_val = 0\n",
    "val_min = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Starting training\n",
    "with summary_writer.as_default():\n",
    "\n",
    "    epochs = parameter_list['epochs']\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        print('\\nStart of epoch %d' %(epoch+1))\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (local_forecast, analysis) in enumerate(train_dataset):\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "            # Open a GradientTape to record the operations run\n",
    "            # during the forward pass, which enables autodifferentiation.\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                pred_analysis = model(local_forecast)\n",
    "\n",
    "                #Calculating relative loss\n",
    "                loss = loss_func(analysis, pred_analysis)\n",
    "\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "\n",
    "            metric_train(analysis, pred_analysis)\n",
    "\n",
    "            # Log of validation results  \n",
    "            if (step % parameter_list['log_freq']) == 0:\n",
    "                print('Training loss (for one batch) at step %s: %s' % (step+1, float(loss)))\n",
    "                print('Seen so far: %s samples' % ((global_step) * parameter_list['batch_size']))\n",
    "                \n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_acc = metric_train.result()\n",
    "        print('Training acc over epoch: %s \\n' % (float(train_acc)))\n",
    "        print('Seen so far: %s samples' % ((step + 1) * parameter_list['batch_size']))\n",
    "\n",
    "        if not(epoch % parameter_list['summery_freq']):\n",
    "            tf.summary.scalar('Loss_total', loss, step= epoch)\n",
    "            tf.summary.scalar('Train_RMSE', train_acc, step= epoch)\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        metric_train.reset_states()\n",
    "\n",
    "        #Code for validation at the end of each epoch\n",
    "        for step_val, (local_forecast_val, analysis_val) in enumerate(val_dataset):\n",
    "\n",
    "            global_step_val += 1\n",
    "\n",
    "            pred_analysis_val = model(local_forecast_val)\n",
    "\n",
    "            val_loss = loss_func(analysis_val, pred_analysis_val)\n",
    "            metric_val(analysis_val, pred_analysis_val)\n",
    "\n",
    "            if (step_val % parameter_list['log_freq']) == 0:\n",
    "                print('Validation loss (for one batch) at step %s: %s' % (step_val, float(val_loss)))\n",
    "                \n",
    "        val_acc = metric_val.result()\n",
    "        print('Validation acc over epoch: %s \\n' % (float(val_acc)))\n",
    "        print('Seen so far: %s samples\\n' % ((step_val + 1) * parameter_list['batch_size']))\n",
    "        \n",
    "        if not(epoch % parameter_list['summery_freq']):\n",
    "            tf.summary.scalar('Loss_total_val', val_loss, step= epoch)\n",
    "            tf.summary.scalar('Val_RMSE', metric_val.result(), step= epoch)\n",
    "            \n",
    "        # Reset training metrics at the end of each epoch\n",
    "        metric_val.reset_states()\n",
    "\n",
    "        checkpoint.epoch.assign_add(1)\n",
    "        if int(checkpoint.epoch + 1) % parameter_list['num_epochs_checkpoint'] == 0:\n",
    "            save_path = manager.save()\n",
    "            print(\"Saved checkpoint for epoch {}: {}\".format(int(checkpoint.epoch), save_path))\n",
    "            print(\"loss {:1.2f}\".format(loss.numpy()))\n",
    "\n",
    "        if math.isnan(val_acc):\n",
    "            print('Breaking out as the validation loss is nan')\n",
    "            break                \n",
    "\n",
    "        if (epoch > 19):\n",
    "            if not (epoch % parameter_list['early_stop_patience']):\n",
    "                if not (val_min):\n",
    "                    val_min = val_acc\n",
    "                else:\n",
    "                    if val_min > val_acc:\n",
    "                        val_min = val_acc\n",
    "                    else:\n",
    "                        print('Breaking loop as validation accuracy not improving')\n",
    "                        save_path = manager.save()\n",
    "                        print(\"Saved checkpoint for epoch {}: {}\".format(int(checkpoint.epoch), save_path))\n",
    "                        print(\"loss {:1.2f}\".format(loss.numpy()))\n",
    "                        break\n",
    "\n",
    "        print('Time for epoch (in minutes): %s' %((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
